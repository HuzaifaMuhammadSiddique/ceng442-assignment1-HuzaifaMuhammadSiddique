Group members:
- Huzaifa Muhammad Siddique

Azerbaijani Text Preprocessing and Word Embeddings (CENG 442 â€“ Assignment 1)

1) Data & Goal
This homework project focused on the cleaning, pre-processing, and preparing of five datasets in the Azerbaijani language. The NLP task that was to be performed on these datasets was sentiment analysis. The goal was to normalize and merge these datasets, build a domain-aware combined corpus, and train both Word2Vec and FastText embeddings on the cleaned text.

Datasets
Following are the five datasets that were cleaned and preprocessed:
- `labeled-sentiment.xlsx` â€“ 3-class dataset (columns: text, sentiment). The sentiment values were 'positive', 'neutral' or 'negative'.
- `test__1_.xlsx` â€“ binary dataset (columns: text, label). The label values were either 0 or 1.
- `train__3_.xlsx` â€“ binary dataset (columns: text, label). The label values were either 0 or 1.
- `train-00000-of-00001.xlsx` â€“ 3-class dataset (columns: text, labels). The labels values were 'positive', 'neutral' or 'negative'.
- `merged_dataset_CSV__1_.xlsx` â€“ binary dataset (columns: text, labels). The labels values were either 0 or 1.

All labels were mapped to numeric values:
- Negative â†’ 0.0
- Neutral â†’ 0.5
- Positive â†’ 1.0

The neutral value (0.5) was used to represent intermediate sentiment polarity, which improves embedding quality and downstream regression-based sentiment tasks. When the sentiment is neither positive nor negative we use the neutral value. Or if the sentiment is somewhere between negative and positive, we use the neutral sentiment.




2) Preprocessing

All of the datasets that we had were cleaned and preprocessed using an Azerbaijani-aware normalization pipeline. Here is a one-paragraph summary of all of the rules that were applied:
Azerbaijani letters like 'Ä°' and 'I' were converted to lowercase. So 'Ä°' became 'i' and  'I' became 'Ä±'. Many replacements were made. So the word 'Ä°ctimai' would become 'ictimai'. And the word 'QABIL' became 'qabÄ±l'.  Like all of the URLs were replaced with the exact text 'URL'. For example a URL like 'https://aybuzem.aybu.edu.tr/' would be changed to just the text 'URL'. Emails were also replaced with the exact text 'EMAIL'. So for example the email '22050141041@aybu.edu.tr' became 'EMAIL'. Phone numbers were also replaced. For phone numbers, the text 'PHONE' was used. For example if we had the phone number '0500-123-45-67' it would simply be replaced with 'PHONE'. Furthermore, mentions like @huzaifa123 were replaced with the text 'USER'. Numbers were replaced with the exact text '<NUM>'. So the text '1975 amerkalÄ±lar gÃ¶r necÉ™ avam olublar gÉ™lsin burda satsÄ±n gÃ¶rÃ¼m o daÅŸÄ±' became '<NUM> amerkalÄ±lar gÃ¶r necÉ™ avam olublar gÉ™lsin burda satsÄ±n gÃ¶rÃ¼m o daÅŸÄ±'. HTML tags like '\<div\>' were also removed. Redundant punctuations were also excluded. We also collapsed repeated characters for example the word 'Ã§ooooox' became 'Ã§ox'. So if in a word, a character was repeated three or more times, it was reduced to two. We also went forward to deasciify common forms like converting 'cox' to 'Ã§ox'. Hashtag splitting was also performed. So the hashtag '#QarabagIsBack' became 'qarabag is back'. Azerbaijani letters were not removed. We also removed single-letter tokens like 'a' or 'l'. The single-letter tokens 'o' and 'e' were not removed because they have a specific meaning in the Azerbaijani language. Most importantly, duplicate and empty rows were removed. Extra spaces were also removed.


After applying each of these rules, each of the datasets were exported as a  two-column Excel file:  
The first column was `cleaned_text` (normalized string) and the second column was `sentiment_value` (float).








3) Mini Challenges


All of the following Mini Challenges were implemented: 
1. Hashtag splitting was performed. So '#QarabagIsBack' was converted to 'qarabag is back'. We used a regular expression to remove the `#` symbol and we splitted camelCase hashtags into readable tokens. The main observation for this is that it improved the tokenization of social media texts, allowing multi-word hashtags to contribute meaningful words to embeddings (e.g., â€œqarabagâ€ and â€œbackâ€ instead of a single combined token).

2. Emoji Mapping was performed. Emojies were mapped to words like 'EMO_POS' which told us that the emoji was positive and 'EMO_NEG' which told us the emoji expressed a negative expression. We used a  small dictionary (`EMO_MAP`)  that mapped emojis to sentiment placeholders. For instance, ğŸ˜Š became `EMO_POS` and ğŸ˜¡ became `EMO_NEG`. The most crucial observation for this is that it  preserved the emotional content of text generated by the user which would otherwise be lost during cleaning. It also allowed sentiment models to recognize emotional cues even when expressed through symbols.

3. Stopword Research was also performed. The following Azerbaijani words were proposed and included in the  stopwords list: "vÉ™","ilÉ™","amma","ancaq","lakin","ya","hÉ™m","ki","bu","bir","o","biz","siz","mÉ™n" ,"sÉ™n","orada","burada","bÃ¼tÃ¼n","hÉ™r","artÄ±q","Ã§ox","az","É™n","dÉ™","da","Ã¼Ã§Ã¼n".
For the implementation, an Azerbaijani stopword list was compared with a Turkish list. Many potential stopword candidates were identified , but negators such as `yox`, `deyil`, `heÃ§`, `qÉ™tiyyÉ™n`, and `yoxdur` were intentionally kept. By default, stopword removal was kept optional because excessive filtering reduced sentiment-bearing words. However, this comparison helped highlight the overlap between Azerbaijani and Turkish function words.



4. Negation Scope was also implemented. Negators like 'yox', 'deyil', 'heÃ§', 'qÉ™tiyyÉ™n' and'yoxdur' marked the next 3 tokens with `_NEG` e.g., deyil yaxÅŸÄ± film became  deyil yaxÅŸÄ±_NEG film_NEG. For the implementation part, the model detected these negator words marked the next three tokens with a `_NEG` suffix. And we can observe that  this improved the modelâ€™s ability to capture sentiment reversals. For example, embeddings of â€œyaxÅŸÄ±_NEGâ€ diverged from â€œyaxÅŸÄ±â€, making negative contexts more distinguishable in the vector space.




5. Deasciification of the text was performed. Common ASCII forms were corrected. For example, cox became Ã§ox and 'yaxsi' became 'yaxÅŸÄ±'. For the purposes of implementation, a  dictionary named 'SLANG_MAP' was used that converted ASCII-based forms to their Azerbaijani equivalents. This ultimately corrected common social media spelling variations and reduced vocabulary fragmentation, and also ensured that semantically identical words were mapped to the same embedding vector.





4) Domain-Aware Features

The text was classified under which domain each of the texts falls. Four domains were taken under consideration: 'news', 'social', 'reviews' and 'general'. Domain was detected using some rules, then domain-specific normalization was applied and all of the text was combined in a text file with the domain labels. 
The main purpose to implement domain awareness was to find out the stylistic differences between texts.

The following rules were used to detect the domain. The rules were based on keywords. 

1. If one of the following words were used in the text, it most likely belonged to the domain 'news':
- apa
- trend
- azertac
- reuters
- bloomberg
- dha
- aa

Most of these are names of famous news agencies.

2. If the text contained emojis, hashtags or mentioned someone using the '@' symbol, then it most likely belonged to the 'social' domain.

3. If the text contained one of the following words then it most likely was of the domain 'reviews':
- azn
- manat
- qiymÉ™t
- Ã§ox yaxÅŸÄ±
- Ã§ox pis

If the text does not comply with any of these given rules then it belongs to the 'general' domain.

After the domain was identified, domain-specific normalization rules were applied to get specialized expressions and patterns unique to each type of text. For example, within the reviews domain, price mentions such as '45 azn' or '25 manat' were replaced with a standardized token <PRICE>, and star ratings like '5 ulduz' were converted to <STARS_5>. Similarly, highly polar phrases such as 'Ã§ox yaxÅŸÄ±' and 'Ã§ox pis' were normalized to <RATING_POS> and <RATING_NEG> respectively, providing clear sentiment cues to the embedding model. These replacements helped us in unifying how numeric and linguistic sentiment expressions are represented, and thus ensured that words like 'bahalÄ±' and 'ucuz' are associated with consistent contextual signals.

Finally, the cleaned sentences were exported into a single text corpus named 'corpus_all.txt', where each line began with a domain prefix for easy identification. This was done by prepending a short tag in the format 'dom<domain>' to each normalized sentence â€” for instance:
- domnews hÃ¶kumÉ™t qÉ™rar qÉ™bul etdi,
- domsocial Ã§ox gÃ¶zÉ™l mahnÄ±dÄ±r EMO_POS,
- domreviews mÉ™hsul Ã§ox keyfiyyÉ™tlidir.







5) Embeddings

We trained two embedding models named 'Word2Vec' and 'FastText' on the combined, domain-tagged Azerbaijani corpus ('corpus_all.txt'). The main goal was to learn distributed word representations capable of capturing semantic and sentiment relationships across multiple domains (news, reviews, social media). The corpus contained over 120,000 cleaned sentences.

--Training Settings--

Both of the models were trained using identical parameters to ensure a fair comparison.

Here is a table that gives details about the exact training settings.

|   Model  | Architecture | vector_size | window | min_count |sg | epochs |
|----------|--------------|-------------|--------|-----------|---|--------|
| Word2Vec |   Skip-Gram  |     300     |    5   |     3     | 1 |   10   |
| FastText |   Skip-Gram  |     300     |    5   |     3     | 1 |   10   |

Both embeddings were trained using the 'gensim' library and saved were saved to the folder 'embeddings/'. We can see that FastTextâ€™s subword-level modeling allowed it to generate vectors for the words that are out-of-vocabulary (OOV) words. It does this by averaging character n-grams. On the other hand, the Word2Vec model only learned vectors for tokens that were seen during its training.


--Evaluation Results--

We can summarize the Lexical Coverage (per dataset) for each of the models in the following table. Lexical coverage is the words from our original text that our model knows (i.e. it can make vector embeddings for those words).

| Dataset                     | Word2Vec | FastText |
|-----------------------------|----------|----------|
| labeled-sentiment_2col      |   0.932  |   0.932  |
| test__1__2col               |   0.987  |   0.987  |
| train__3__2col              |   0.990  |   0.990  |
| train-00000-of-00001_2col   |   0.943  |   0.943  |
| merged_dataset_CSV__1__2col |   0.949  |   0.949  |

Observation: We can clearly see that the lexical coverage for both of the models is EXACTLY the same for each dataset. This indicates that the vocabulary was consistent and sufficiently learned by both of the architectures. Moreover, it reflects the effectiveness of text normalization and deasciification in minimizing rare word forms.


--Coverage by Domain--

We have also included analysis about coverage by domain of each model according to the datasets provided. Coverage by domain basically means how well each of the model represents the words within each detected domain of the corpus. It is a better version of lexical coverage.

Here is the summarized data: 

| Domain     | Word2Vec  | FastText |
|------------|-----------|----------|
| domnews    |   0.930   |   0.930  |
| domsocial  |   0.714   |   0.714  |
| domreviews |   0.953   |   0.953  |
| domgeneral |   0.956   |   0.956  |

Observation: We can see that the values are again exactly same for both of the models. Furthermore, the coverage was highest in 'reviews' and 'general' domains mostly due to standardized language that is used in these domains. Whereas, for the 'social' domain, the coverage dropped to approximately 71% mainly because of the noisy or informal tokens for example emojis, slang, user handles, etc. The identical results suggest that most subword advantages of FastText were neutralized by strong preprocessing.


--Synonym and Antonym Similarities--

We have  also analyzed the similarities for synonyms and antonyms. Here is the summarized table to represent the results:

| Metric                                          | Word2Vec | FastText |
|-------------------------------------------------|----------|----------|
| Synonyms (yaxÅŸÄ±â€“É™la, bahalÄ±â€“qiymÉ™tli, pisâ€“zÉ™if) |   0.290  |   0.369  |
| Antonyms (yaxÅŸÄ±â€“pis, bahalÄ±â€“ucuz, mÃ¼sbÉ™tâ€“mÉ™nfi) |   0.396  |   0.474  |

Observation: We can clearly observe that the values for the FastText model are higher than the values for the WOrd2Vec model. This is mainly because the FastText model is sensitive to the subword morphology. Unexpectedly, the antonym similarity scores were higher than synonym scores, this likely happened due to sentiment words appearing in overlapping contexts for example both 'yaxÅŸÄ±' and 'pis' frequently co-occur with the same product or topic words. 



--Nearest Neighbor Analysis--

| Word   |                   Word2Vec Top Neighbors                    | FastText Top Neighbors |
|--------|-------------------------------------------------------------|------------------------|
| yaxÅŸÄ±  | iyi, `<RATING_POS>`, yaxshi, awsome, yaxsÄ±                  | yaxÅŸÄ±Ä±, yaxÅŸÄ±kÄ±, yaxÅŸÄ±ca, yaxÅŸÄ±, yaxÅŸÄ±ya |
| pis    | `<RATING_NEG>`, gÃ¼nd, vÉ™rdiÅŸlÉ™rÉ™, bugunki, lire             | piis, pisdii, pi, pisik, pisi |
| bahalÄ± | metallarla, yaxtalarÄ±, villalarÄ±, portretlerinÉ™, qabardÄ±lÄ±r | bahalÄ±Ä±, bahalÄ±sÄ±, bahalÄ±q, baharlÄ±, pahalÄ± |
| film   | filim, dublaj, dublyaj, filmi, filmdir                      | films, filmax, filmi, filme, filmdÄ±ki |
| mahnÄ±  | mahnÄ±nÄ±, oxudur, qoÅŸuram, mahnÄ±nÄ±n, mahnÄ±yÄ±                 | mahnÄ±yÄ±, mahnÄ±nÄ±, mahnÄ±ya, mahni, mahnÄ±sÄ± |

Observations:
For the Word2Vec modelâ€™s results, we can see that the neighbors show better semantic coherence because the neighbor words are related in meaning e.g., for the word 'film', we have the neighbors:  filim, dublaj, filmdir. This has happened due to context-based co-occurrence learning.  
For the FastText model, we can see that it generated morphological variants for the tokens. Like for the token 'mahnÄ±', we have the neighbors: mahnÄ±yÄ±, mahnÄ±nÄ±, mahnÄ±ya. This reflects that the focus of this model is on subword patterns rather than word meaning.  
In general, the FastText model showed stronger morphological generalization but weaker semantic grouping, whereas the Word2Vec model captured more meaningful contextual associations.



6) Lemmatization (Optional)

Lemmatization was not applied in this project. 




7) Reproducibility

In order to ensure that the preprocessing, embedding training, and evaluation steps can be reproduced on any machine, all code was developed and executed in a controlled Python environment. 
The versions, execution order, and expected directory structure are listed below.

--Environment and Versions--
The Windows 11 (64-bit) operating system was used. The process 13th Gen Intel(R) Core(TM) i5-1335U (1.30 GHz) with 12 GB RAM was used. The version of python that was used is '3.11.8'. The following libraries were used: pandas, gensim, ftfy, scikit-learn, openpyxl and regex. For the random seeds, default seeds from gensim were used but the results may vary slightly on retraining. As far as the hardware is concerned, CPU was used for training; no GPU is required.

It is important to note that this project does not depend on any OS-specific paths and can run on Linux or macOS as long as the same Python and library versions are installed.

--Folder Structure--
The way the directory was organized is as follows:

main_folder/
â”‚

â”œâ”€â”€ main_pipeline.py # Data cleaning & corpus creation

â”œâ”€â”€ train_embeddings.py #  Word2Vec / FastText training

â”œâ”€â”€ compare_embeddings.py #  Evaluation & comparison

â”œâ”€â”€ embeddings/ # Saved .model files

â””â”€â”€ corpus_all.txt # Combined domain-tagged corpus

The input excel files and the output excel files were directly loaded/created into the main_folder.


--How to Reproduce the Results--

Here is an explanation of how you can set up the entire environment and run the code. You need to run the following commands in the terminal in the specified order:


`pip install pandas gensim ftfy scikit-learn openpyxl regex`  # installs dependencies

`python main_pipeline.py` # cleans the datasets and creates two-column files and corpus_all.txt

`python train_embeddings.py` # Trains the models

`python compare_embeddings.py` # compares the model results




8) Conclusions

Both Word2Vec and FastText models successfully learned Azerbaijani word representations from the cleaned and domain-tagged corpus. However, their behavior differed in how they captured the meaning and morphology.

--Overall Comparison--

Word2Vec was able to achieve stronger semantic precision in nearest-neighbor lists, grouping conceptually related tokens.
FastText did produce slightly higher synonym and antonym similarity scores but it mostly generated morphological variants. This shows its subword mechanism captured character-level structure rather than contextual meaning.  
Ultimately, this aligns with our expectations: FastText performs better on morphologically rich or noisy data, whereas Word2Vec is better at pure semantic relations once text has been normalized.


--Quantitative Findings--

- Lexical coverage: It was very high (93â€“99 %) across all datasets for both of the models. This confirms that preprocessing effectively unified spelling and removed rare tokens.  
- Domain-wise coverage: This highlighted differences in the text style: social domain had the lowest coverage which reflects noisy, emoji-rich posts, while reviews and news data had a really high coverage (0.93).  
- Similarity scores: These were modest which suggests contextual overlap between positive and negative expressions in the datasets. This means that both models captured usage patterns more than true sentiment polarity.  
- Nearest-neighbor analysis: This confirmed that FastText generalized morphological families whereas Word2Vec clustered semantically meaningful words.


--Interpretation--

Given the highly normalized corpus, Word2Vecâ€™s context-based training provided clearer semantic neighborhoods and better interpretability for sentiment and topic analysis tasks. 
FastText, on the other hand, did not add much advantage in coverage because most spelling variants were already standardized during preprocessing. Nevertheless, FastText remains beneficial for future, noisier social-media datasets where unseen or creative word forms are common.


--Next Steps--

- Incorporate lemmatization or morphological segmentation to reduce redundant inflected forms and further clarify semantic relationships.  
- Expand the corpus with larger and more diverse Azerbaijani sources for example news archives, product reviews, microblogs.  
- Evaluate embeddings on downstream sentiment classification or semantic similarity tasks for quantitative benchmarking.  
- Optionally experiment with contextual embeddings like BERT or multilingual transformers to compare static vs. contextualized vector representations.

In summary, the Word2Vec model performed slightly better for this cleaned Azerbaijani datasets due to its strong semantic grouping, while the FastText embedding model did offer robustness to subword variation but less semantic clarity. The pipeline remains fully extensible for deeper morphological and domain-specific exploration.



